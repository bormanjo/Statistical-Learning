---
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```



# FE590.  Assignment #2.


## `r format(Sys.time(), "%Y-%m-%d")`




# Instructions
In this assignment, you should use R markdown to answer the questions below. Simply type your R code into embedded chunks as shown above.
When you have completed the assignment, knit the document into a PDF file, and upload both the .pdf and .Rmd files to Canvas.
```{r}
CWID = 10402229 #Place here your Campus wide ID number, this will personalize
#your results, but still maintain the reproduceable nature of using seeds.
#If you ever need to reset the seed in this assignment, use this as your seed
#Papers that use -1 as this CWID variable will earn 0's so make sure you change
#this value before you submit your work.
personal = CWID %% 10000
set.seed(personal)#You can reset the seed at any time in your code, but please always set it to this seed.

library(magrittr) # Piping operators

```
# Question 1 
Use the Auto data set from the textbook's website. When reading the data, use the options as.is = TRUE
and na.strings="?". Remove the unavailable data using the na.omit() function.

```{r}
df = read.table("Auto.data", as.is = T, na.strings = "?", header = T)
df = na.omit(df)
```


## 1. List the names of the variables in the data set.

```{r}
names(df)
```


## 2. The columns origin and name are unimportant variables. Create a new data frame called cars that contains none of these unimportant variables

```{r}
cars = df[, !(colnames(df) %in% c("origin", "name"))]
names(cars)
```

## 3. What is the range of each quantitative variable? Answer this question using the range() function with the sapply() function e.g., sapply(cars, range). Print a simple table of the ranges of the variables. The rows should correspond to the variables. The first column should be the lowest value of the corresponding variable, and the second column should be the maximum value of the variable. The columns should be suitably labeled.

```{r}
sapply(cars, range) %>% 
  t() %>% 
  set_colnames(c("Min", "Max"))
```

## 4. What is the mean and standard deviation of each variable? Create a simple table of the means and standard deviations.

```{r}
means <- 
  cars %>%
  sapply(mean)

sds <- 
  cars %>%
  sapply(sd)

tbl <- 
  rbind(means, sds) %>% 
  t() %>% 
  set_colnames(c("Mean", "Std Dev"))

tbl
```

## 5. Create a scatterplot matrix that includes the variables mpg, displacement, horsepower, weight, and acceleration using the pairs() function.

```{r}
pairs(cars[,c("mpg", "displacement", "horsepower", "weight", "acceleration")])
```

## 6.  Using the regsubsets function in the leaps library, regress mpg onto

\begin{itemize}

\item displacement
\item horsepower
\item weight
\item acceleration

\end{itemize}

```{r}
library(leaps)

model <- leaps::regsubsets(data = cars, mpg ~ displacement + horsepower + weight + acceleration)
```

## 7. Print a table showing what variables would be selected using best subset selection for all predictors (displacement, horsepower, weight, acceleration) up to order 2 (i.e. weight and weight^2).

```{r}
model <- leaps::regsubsets(data = cars, 
                        mpg ~ displacement + horsepower + weight + acceleration + poly(displacement, 2) + poly(horsepower, 2) + poly(weight, 2) + poly(acceleration, 2))
summary(model)$which
```

### a. What is the most important variable affecting fuel consumption?

```{r}
summary(model)$which %>% apply(2, sum)
```

Out of all the models constructed, the displacement^2 was included the most often therefore one could argue that the importance of this variable is reflected in its frequency.

### b. What is the second most important variable affecting fuel consumption?

Based on the previous notion of importance, horsepower and horsepower^2 are tied for the second most important variable.

### c. What is the third most important variable affecting fuel consumption?

The next most important variable is weight^2. Technically this would be the fourth most important variable, given that two variables are tied for second.

# Question 2 

This exercise involves the Boston housing data set.

## 1. Load in the Boston data set, which is part of the MASS library in R. The data set is contained in the object Boston. Read about the data set using the command ?Boston. How many rows are in this data set? How many columns? What do the rows and columns represent?

```{r}
library(MASS)
dim(Boston)
```

The dimensions of the Boston data set are 506 (rows) by 14 (cols) representing the observations (suburbs) and variables, respectively.


## 2. Do any of the suburbs of Boston appear to have particularly high crime rates?


```{r}
boxplot(Boston$crim)
Boston$crim %>% is_greater_than(50) %>% sum
```

The boxplot above shows a visual representation of the `summary()` function applied to the `crim` variable. There are a number of suburbs (observations) that have crime rates above 50 crimes per capita, specifically 4 suburbs. Note that the number of 50 crimes per capita is an arbitrary number, but to me the rate of 50 crimes per person living in a suburb is a *significantly* high number.

## Tax rates?

```{r}
boxplot(Boston$tax)
Boston$tax %>% is_greater_than(650) %>% sum()
```

There are 137 suburbs with tax rates above what is, roughly, the third quantile rate (~650) that can be considered particularly high tax rates

## Pupil-teacher ratios?

```{r}
boxplot(Boston$ptratio)
Boston$ptratio %>% is_greater_than(20) %>% sum
```

Using a similar methodology, there are 201 suburbs with a pupil-teacher ratio above 20 (roughly the third quantile). However, as can be seen in the boxplot, there are no extraordinary values in regards to the variable. In my opinion, these pupil-teacher ratios are relatively normal/small.

## Comment on the range of each predictor.

**Crime Rate** - The crime rate has a very sporadic distribution across a range of values from 0 to over 100.

**Tax Rate** - The tax rate has a very wide distribution ranging from 200 to 700.

**Pupil-Teacher Ratio** - Like the tax rate, the Pupil-Teacher ration has a wide distribution and ranges from ~12 to 22.

## 3. How many of the suburbs in this data set bound the Charles river?

```{r}
Boston$chas %>% sum()
```

There are 35 along the Charles river.

## 4. What is the median pupil-teacher ratio among the towns in this data set?

```{r}
Boston$ptratio %>% median
```

The median pupil-teacher ratio is 19.05

## 5. In this data set, how many of the suburbs average more than seven rooms per dwelling?

```{r}
Boston %>% subset(rm > 7) %>% nrow()
```

There are 64 suburbs that average more than 7 rooms per dwelling.

## More than eight rooms per dwelling?

```{r}
Boston %>% subset(rm > 8) %>% nrow()
```

There are 13 that average more than 8


# Question 3 

This question should be answered using the Weekly data set, which is part of the ISLR package. This data contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

## 1. What does the data represent?

```{r}
library(ISLR)
summary(Weekly)
```

The data represents a time series of financial percentage returns containing variables indicating a k-day lag, that is the value of the time series k-days ago. Additionally, it also contains a categorical variable that indicates the direction.

## 2. Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = "binomial")
summary(model)
```

At the 99% level, the second lag is statistically significant


## 3. Fit a logistic regression model using a training data period from 1990 to 2008, using the predictors from the previous problem that you determined were statistically significant. Test your model on the held out data (that is, the data from 2009 and 2010) and express its accuracy.


```{r}
test <- Weekly[Weekly$Year %in% 2009:2010,]
train <- Weekly[Weekly$Year %in% 1990:2008,]

model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = train, family = "binomial")
summary(model)

pred <- predict(model, test, type = "response") > 50
pred.vals <-  as.factor(pred)
summary(pred)
table(test$Direction, pred.vals)

```

## 4. Repeat Part 3 using LDA.

```{r}
#insert r code here
```


## 5. Repeat Part 3 using QDA.

```{r}
#insert r code here
```

## 6. Repeat Part 3 using KNN with K = 1, 2, 3.

```{r}
#insert r code here
```


## 7. Which of these methods in Parts 3, 4, 5, and 6 appears to provide the best results on this data?

```{r}
#insert r code here
```


# Question 4

## Write a function that works in R to gives you the parameters from a linear regression on a data set between two sets of values (in other words you only have to do the 2-D case and your output will be the coefficients beta_0 and beta_1).  Include in the output the standard error of your variables.  You cannot use the lm command in this function or any of the other built in regression models.  For example your output could be a 2x2 matrix with the parameters in the first column and the standard errors in the second column.  For up to 5 bonus points, format your output so that it displays and operates similar in function to the output of the lm command.(i.e. in a data frame that includes all potentially useful outputs)


```{r}
#insert r code here
```

## Compare the output of your function to that of the lm command in R.

```{r}
#insert r code here
```
# Question 5
## Using the Advertising data set (Sales, TV, Radio, Newspaper), do the following:

## 1. Randomly split the data into two different pieces of roughly equal size.
```{r}

```
## 2. Pick one set to run a linear regression to predict sales based on all TV and Radio, and then test your accuracy using the other set
```{r}

```
## 3. Repeat the previous problem using all three predictors (including newspaper).  What do you determine from this result?
```{r}

```
## 4. Determine the LOOCV error for the linear regression using all three predictors.
```{r}

```